## 原始积累

首先先积累一些基础知识，比如说一些经典的网络架构，比如说经典的 transformer，然后以及各个大的领域主要 focus 的东西，包括说一些这方面比较经典的绝活，诸如对比学习里的那个 InstDisc 等等。

## 认祖归宗

首先问一问老师，看看自己做的这个领域有没有经典文章，也就是一些老祖宗级别的 baseline，比如说我有读过 mean teacher，在半监督领域挺知名的，这类论文的一个特点是，你去最新的论文的 introduction 里面转一圈，基本还会提到它们。

这些论文的思想都是十分实用的，了解这些论文算是进阶版的知识积累。一个“稳啦”的开局是，有大佬做过相关的综述或者论文串讲，别找比较野鸡的，对于方法的故事后面真正 work 的东西是什么有了解很重要。

## GPT，启动！

接下来你终于来到了论文粗度的起点，所以说还不快去注册一个 gpt，为了以防有打广告的嫌疑，仅是提一嘴，假如觉得注册很麻烦，买一个号很方便的，几块钱就够，被封了大不了再买一个。

一般来说，我们认为中文的知识密度是大于英文的，而且作为母语，也比较好理解。

直接全部复制摘要，问 GPT，翻译以下内容，然后你就知道它干了什么。甚至因为排版的问题，可能你直接复制会有一定乱的格式，都不用管，gpt can handle this。摘要是一篇文章的精华，告诉了你他们的 work 是什么。

之后看 introduction，作为一个领域刚刚入门的人，可以进行一个迭代学习，看到不会的之前的工作，向前回溯，introduction 是论文的故事在的地方，看看可以知道这篇工作的故事走向，可以信一部分，但别完全信。

之后跳到 discussions，老规矩继续 gpt 翻译，不会的东西先问 gpt，再查网上，毕竟都到论文了，网上的东西只会越来越少，尤其是一些下游任务，和它聊久了你就能感觉出来它有什么东西讲的很确信，有什么是在瞎扯。

这时候你已经读完论文了，你可以大言不惭的说你读完了，但是假如说你觉得这个工作很吸引你，那就继续看，related works 就是一个小综述，看看总没坏处，万一写得很好呢。

之后 method 里面别管方法的名字，看这些公式的本质，实在不行问问 gpt 或者去查查，说不定这个方法只是之前的一个别的方法的翻版，毕竟 MLP 都可以叫做 predictor，方法的名字取决于故事，但是它为什么 work，你可能需要自己想一想自己的理解，捋一下数据在它的 pipeline 图里的流向。

在之后，在 experiment 里面主要关注消融实验，方法对比主要是秀肌肉，消融实验能告诉你它可能提出的三五种策略组合起来之后，哪些 work 了。

这样你就彻底读完一篇论文了，它具体有没有前景你的心里也已经有数了。

## 润一下源码

接下来你要是想要去做，和老师谈一谈，然后找到它的代码，作为新手，还是建议大家找有源码的论文，在上面做拓展也方便，不懂的地方依然 gpt 解决，直接每个文件 `ctrl+A` 全部复制，或者分段复制，让 gpt 加中文注释或者简要解释每个函数的功能，之后 `tree` 一下，也给 gpt，方便 gpt 理解一切。

代码也是讲究一个跑通就好，看看上面的 args 里面都有啥，然后看看 dataloader 这部分的循环里面怎么训练的，假如有实现一个 module，看看里面的结构，像是数据处理之类的就不用管了，主要看 idea 是怎么实现的，至于怎么读，这里我要提一个人类的好朋友，gpt......

以上，当然，也不难发现，最重要的是成为一个 gpt chater，每一个人一天和 gpt 说话少于 20 句，都是摸鱼的一天。

Version 0.0.1